{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7fa48058-faa3-47e6-8050-fc92b7f9f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "import shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1c3360d6-0c9c-4de8-af51-db05a240aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def which_zip_is_which_data_source(input_data_folder):\n",
    "    '''\n",
    "    Function that searches the input data folder for zip files and determines \n",
    "    which zipfiles contain the data needed for this task. The function also \n",
    "    identifies the type of data/data source for each of those zipfiles. \n",
    "    For example: 'texas_inrix_npmrds_15min(1).zip' contains the 'NPMRDS from \n",
    "    INRIX (Passenger vehicles)' data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data_folder : STR\n",
    "        String that indicates the folder to be investigated for the zipfiles \n",
    "        containing the raw data\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_paths_dict: DICT\n",
    "        Dictionary that contains information about where the files for each \n",
    "        data source is located. It should be structured as follows:\n",
    "            {'data_origin_1':{'zip_file':'zip_file_1_full_path.zip',\n",
    "                              'raw_data_file':'raw_data_file_name_1.csv'},\n",
    "             'data_origin_2':{'zip_file':'zip_file_2_full_path.zip',\n",
    "                              'raw_data_file':'raw_data_file_name_2.csv'},\n",
    "             ...}\n",
    "    '''\n",
    "    \n",
    "    # Dictionary that is used to match data origin to the RegEx string\n",
    "    dict_for_origin_match = {\n",
    "        'inrix':\n",
    "            '.*INRIX TMC.*',\n",
    "        'npmrds_from_inrix_pass_vehicles':\n",
    "            '.*NPMRDS from INRIX \\(Passenger vehicles\\).*',\n",
    "        'npmrds_from_inrix_trucks':\n",
    "            '.*NPMRDS from INRIX \\(Trucks\\).*',\n",
    "        'npmrds_from_inrix_trucks_and_passveh':\n",
    "            '.*NPMRDS from INRIX \\(Trucks and passenger vehicles\\).*'}\n",
    "    \n",
    "    \n",
    "    # Find all zip files in input folder\n",
    "    folder_files = [f for f in os.listdir(input_data_folder) if \n",
    "                    os.path.isfile(os.path.join(input_data_folder, f))]\n",
    "    \n",
    "    # Filter only zipfiles\n",
    "    folder_zips = list(filter(re.compile(\".*zip$\").match, folder_files))\n",
    "    \n",
    "    # Dictionary that will store the output\n",
    "    data_paths_dict = {}\n",
    "    \n",
    "    # Looping over zipfiles\n",
    "    for this_zip_file in folder_zips:\n",
    "        with zipfile.ZipFile(os.path.join(input_data_folder, \n",
    "                                          this_zip_file)) as this_zip:\n",
    "            \n",
    "            # Checking if this is a data extract from RITIS' massive data downloader\n",
    "            files_in_zip = this_zip.namelist()\n",
    "            if 'Contents.txt' in files_in_zip:\n",
    "                \n",
    "                #Extracting the name of the raw data CSV file inside this zipfile\n",
    "                raw_data_file = [this_file for this_file in files_in_zip if \n",
    "                                 this_file !='Contents.txt' and \n",
    "                                 this_file !='TMC_Identification.csv'][0]\n",
    "                \n",
    "                # Performing a RegEx search to find which data source this \n",
    "                # zipfile originally came from\n",
    "                with this_zip.open('Contents.txt','r') as content_file:\n",
    "                    this_content = content_file.readline().decode('utf-8')\n",
    "                    for this_data_origin, this_regex_string in (\n",
    "                            dict_for_origin_match.items()):\n",
    "                        regex_search = re.match(this_regex_string,this_content)\n",
    "                        if regex_search:\n",
    "                            data_paths_dict[this_data_origin] = {\n",
    "                                'zip_file':os.path.join(input_data_folder,\n",
    "                                                        this_zip_file),\n",
    "                                'raw_data_file':raw_data_file}\n",
    "    return data_paths_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b615224d-d769-45cb-bbe1-77c2ec5ac95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_paths_dict = which_zip_is_which_data_source(r'C:\\Users\\MUSSAHAK\\Downloads\\HDOT Freight Plan NPMRDS\\raw_data\\15 mins Interval\\2017')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "158991ad-8be0-411f-9bde-8f349cdd7f00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'zip_file': 'C:\\\\Users\\\\MUSSAHAK\\\\Downloads\\\\HDOT Freight Plan NPMRDS\\\\raw_data\\\\15 mins Interval\\\\2017\\\\Hawaii_YR2017_15-mins.zip',\n",
       " 'raw_data_file': 'Hawaii_YR2017_15-mins.csv'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths_dict['npmrds_from_inrix_trucks']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "17690014-e67a-45bb-a1e3-2f4abbac9109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hawaii_YR2017_15-mins.csv'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_paths_dict['npmrds_from_inrix_trucks']['raw_data_file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "67106b41-a9f5-4f1e-88f0-3b9061169896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_get_specific_road_segments(data_origin,\n",
    "                                        raw_data_zipfile,\n",
    "                                        raw_data_filename_in_zip,\n",
    "                                        road_str,chunk_size,\n",
    "                                        raw_data_chunks=None,\n",
    "                                        tmc_data_parts=None):\n",
    "    \"\"\"\n",
    "    Function used to read in raw speed and TMC segment data. This function will \n",
    "    likely be called multiple times because of the several different sources of \n",
    "    input files. For example: INRIX, NPMRDS from INRIX, etc.\n",
    "    \n",
    "    INPUT VARIABLES:\n",
    "    ----------------\n",
    "    data_origin: STR \n",
    "       String that characterizes the origin of the data. Sample values: \n",
    "       'inrix','npmrds_from_inrix_trucks', 'npmrds_from_pass_vehicles'\n",
    "    raw_data_zipfile: STR\n",
    "        String that contains the file/folder location of the zipfile to be read\n",
    "    raw_data_filename_in_zip: STR\n",
    "        String that contains the filename of the raw data inside the zipfile\n",
    "    road_str: STR used to filter road segments based on their names. The TMC \n",
    "        segments will be filtered based on whether or not the 'road' column \n",
    "        contains this string. To get the entire dataset back, just use an \n",
    "        empty string ('').\n",
    "    chunk_size: INT\n",
    "        Integer used to determine number of rows read at a time by Pandas' \n",
    "        read_csv method.\n",
    "    raw_data_chunks: LIST \n",
    "        List containing the several chunks of input files thus far.\n",
    "        The first time this function is called, this should just be an empty list.\n",
    "    tmc_data_parts: LIST \n",
    "        List containing the several tmc_data inputs from the multiple times this \n",
    "        function is called.\n",
    "                        \n",
    "    OUTPUT:\n",
    "    -------\n",
    "    output_dict : DICT\n",
    "        Dictionary that contains two values: 'raw_data_chunks' and 'tmc_data_parts':\n",
    "        raw_data_chunks: LIST \n",
    "            List of pd.DataFrames that contain the several chunks of all the \n",
    "            input  data-files, including the chunks created in the current \n",
    "            execution of this method. \n",
    "            Note: It is expected that these chunks will later be concatenated \n",
    "            into one large DataFrame afterwards.\n",
    "        tmc_data_parts: LIST \n",
    "            List of pd.DataFrames containing the analogous TMC data \n",
    "            (i.e., the data in the \"TMC_Identification.csv\" files).\n",
    "    \"\"\" \n",
    "    \n",
    "    if not raw_data_chunks:\n",
    "        raw_data_chunks = []\n",
    "    if not tmc_data_parts:\n",
    "        tmc_data_parts = []\n",
    "    \n",
    "    # Opening the zipfile\n",
    "    with zipfile.ZipFile(raw_data_zipfile) as this_zip:\n",
    "        \n",
    "        # Reading in the TMC data from the zipfile\n",
    "        with this_zip.open('TMC_Identification.csv','r') as tmc_data_file:\n",
    "            tmc_data = pd.read_csv(tmc_data_file, low_memory=False)\n",
    "    \n",
    "        # Adding extra column about data origin and storing the final result\n",
    "        tmc_data['data_origin'] = data_origin\n",
    "        tmc_data['road'] = tmc_data['road'].fillna('')\n",
    "        \n",
    "        # Fixing column names\n",
    "        tmc_data = tmc_data.rename({'tmc':'tmc_code',\n",
    "                                    'intersection':'intersection_',\n",
    "                                    'state':'state_',\n",
    "                                    'type':'type_'},axis=1)\n",
    "        \n",
    "        # Querying main searched road\n",
    "        tmc_data = tmc_data.query(f'road.str.contains(\"{road_str}\")', \n",
    "                                  engine='python')\n",
    "        \n",
    "        # Sometimes, this DataFrame has multiple rows for the same TMC. \n",
    "        # This step is taken to de-duplicate the TMCs data.\n",
    "        tmc_data = (tmc_data\n",
    "                    .sort_values(by=['tmc_code','active_end_date'])\n",
    "                    .reset_index(drop=True))\n",
    "        tmc_data = tmc_data.groupby('tmc_code').last().reset_index()\n",
    "        tmc_data_parts.append(tmc_data.copy())\n",
    "        \n",
    "        # Subset of the TMC data with only the relevant columns\n",
    "        tmc_data_sub = tmc_data[['tmc_code','road','data_origin']]\n",
    "        \n",
    "        # Reading in the raw data in chunks and only keeping segments that \n",
    "        # are related to the main searched road\n",
    "        with this_zip.open(raw_data_filename_in_zip,'r') as tmc_data_file:\n",
    "            with pd.read_csv(tmc_data_file, \n",
    "                             chunksize=chunk_size, \n",
    "                             dtype={'tmc_code':'str'}) as reader:\n",
    "                for raw_data in reader:\n",
    "                    raw_data = raw_data.merge(tmc_data_sub, \n",
    "                                              how='left', on='tmc_code')\n",
    "                    raw_data = raw_data.loc[raw_data.road.notnull()]\n",
    "                    raw_data_chunks.append(raw_data.copy())\n",
    "    \n",
    "    # Since we need to return more than one output, the multiple outputs have \n",
    "    # been added to a dictionary.\n",
    "    output_dict = {'raw_data_chunks':raw_data_chunks,\n",
    "                   'tmc_data_parts':tmc_data_parts}\n",
    "     \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d2c5993c-9c5f-4aea-ad4d-4038e4f777d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_origin = 'npmrds_from_inrix_trucks'\n",
    "raw_data_zipfile = data_paths_dict['npmrds_from_inrix_trucks']['zip_file']\n",
    "raw_data_filename_in_zip = data_paths_dict['npmrds_from_inrix_trucks']['raw_data_file']\n",
    "road_str = ''\n",
    "chunk_size = 100000\n",
    "\n",
    "input_data_folder = r'C:\\Users\\MUSSAHAK\\Downloads\\HDOT Freight Plan NPMRDS\\raw_data\\15 mins Interval\\2022'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d594c22c-ef23-4e79-a7c6-a5abb33d27a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 2min 51s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "output_dict = read_csv_get_specific_road_segments(data_origin,\n",
    "                                        raw_data_zipfile,\n",
    "                                        raw_data_filename_in_zip,\n",
    "                                        road_str,chunk_size,\n",
    "                                        raw_data_chunks=None,\n",
    "                                        tmc_data_parts=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7499e31d-c4eb-47ce-b689-ee332a2841e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_chunks = output_dict['raw_data_chunks']\n",
    "tmc_data_parts = output_dict['tmc_data_parts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d1fb545c-1b38-4a25-9ce7-113760147bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_one_set_of_raw_data(input_data_folder,road_str,chunk_size,data_origin):\n",
    "    '''\n",
    "    Looks into the input folder and reads in the raw data contained in only \n",
    "    one of the zipfiles.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data_folder : STR\n",
    "        String that indicates the folder to be investigated for the zipfiles \n",
    "        containing the raw data\n",
    "    road_str: STR \n",
    "        String used to filter road segments based on their names. This is also \n",
    "        referred to as \"the main searched road\" in other places of this script.\n",
    "        The TMC segments will be filtered based on whether or not the 'road' \n",
    "        column contains this string. To get the entire dataset back, just use \n",
    "        an empty string ('').\n",
    "    chunk_size : INT\n",
    "        Integer that defines the chunk size for Pandas' `read_csv` method.\n",
    "    data_origin: STR \n",
    "       String that characterizes the origin of the data. Sample values: \n",
    "       'inrix','npmrds_from_inrix_trucks', 'npmrds_from_pass_vehicles'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_dict: DICT\n",
    "        Dictionary with two entries: \"main_data\" and \"main_tmc_data\".\n",
    "        main_data : pd.DataFrame\n",
    "            Pandas DataFrame that contains the actual raw speed data for the main \n",
    "            searched road\n",
    "        main_tmc_data : pd.DataFrame\n",
    "            Pandas DataFrame that contains the associated TMC data for all the \n",
    "            TMC segments on the main searched road\n",
    "\n",
    "    '''\n",
    "\n",
    "    # Searching the input folder for zipfiles and determining where the relevant\n",
    "    # raw data files are. This function also tells you which \"data_origin\" is \n",
    "    # associated with each of the zipfiles.\n",
    "    data_paths_dict = which_zip_is_which_data_source(input_data_folder)\n",
    "    \n",
    "    \n",
    "    # This is an empty list that will store the DataFrame chunks from reading \n",
    "    # in the raw speed data.\n",
    "    raw_data_chunks = []\n",
    "    \n",
    "    # This is an empty list that will store the DataFrames containing the TMC-\n",
    "    # segment  link data that is associated with each data source (i.e., the \n",
    "    # data in the \"TMC_Identification.csv\" files)\n",
    "    tmc_data_parts = []\n",
    "    \n",
    "    \n",
    "    raw_data_zipfile = data_paths_dict[data_origin]['zip_file']\n",
    "    raw_data_filename_in_zip = data_paths_dict[data_origin]['raw_data_file']\n",
    "    results_dict = read_csv_get_specific_road_segments(\n",
    "                             data_origin=data_origin,\n",
    "                             raw_data_zipfile=raw_data_zipfile,\n",
    "                             raw_data_filename_in_zip=raw_data_filename_in_zip,\n",
    "                             road_str=road_str,\n",
    "                             chunk_size=chunk_size,\n",
    "                             raw_data_chunks=raw_data_chunks,\n",
    "                             tmc_data_parts=tmc_data_parts)\n",
    "    raw_data_chunks = results_dict['raw_data_chunks']\n",
    "    tmc_data_parts = results_dict['tmc_data_parts']\n",
    "    \n",
    "    # Concatenating all raw data chunks into one single DataFrame\n",
    "    main_data = pd.concat(raw_data_chunks, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    # Making sure there are no duplicates. If there are, they are averaged out.\n",
    "    #main_data = main_data.groupby(['data_origin','tmc_code','measurement_tstamp']).mean().reset_index()\n",
    "    main_data = main_data.drop_duplicates(subset=['data_origin','tmc_code','measurement_tstamp']).reset_index(drop=True)\n",
    "    \n",
    "    # Concatenating all TMC data parts into one single DataFrame\n",
    "    main_tmc_data = pd.concat(tmc_data_parts, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    # Since we need to return more than one output, the multiple outputs have \n",
    "    # been added to a dictionary.\n",
    "    output_dict = {'main_data':main_data,\n",
    "                   'main_tmc_data':main_tmc_data}\n",
    "    \n",
    "    return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a0c8508a-6e96-4433-90b2-bd14aaa66c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 1s\n",
      "Wall time: 4min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test = read_one_set_of_raw_data(input_data_folder,road_str,chunk_size,data_origin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "c7b126aa-4128-433c-89dc-55dbe238f7f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmc_code</th>\n",
       "      <th>measurement_tstamp</th>\n",
       "      <th>speed</th>\n",
       "      <th>average_speed</th>\n",
       "      <th>reference_speed</th>\n",
       "      <th>travel_time_seconds</th>\n",
       "      <th>data_density</th>\n",
       "      <th>road</th>\n",
       "      <th>data_origin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 00:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 00:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 00:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039515</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 22:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039516</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039517</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 23:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039518</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 23:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039519</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 23:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74039520 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tmc_code   measurement_tstamp  speed  average_speed  \\\n",
       "0         126N04514  2022-01-01 00:00:00    NaN            NaN   \n",
       "1         126N04514  2022-01-01 00:15:00    NaN            NaN   \n",
       "2         126N04514  2022-01-01 00:30:00    NaN            NaN   \n",
       "3         126N04514  2022-01-01 00:45:00    NaN            NaN   \n",
       "4         126N04514  2022-01-01 01:00:00    NaN            NaN   \n",
       "...             ...                  ...    ...            ...   \n",
       "74039515  126P04663  2022-12-31 22:45:00    NaN           46.0   \n",
       "74039516  126P04663  2022-12-31 23:00:00    NaN           44.0   \n",
       "74039517  126P04663  2022-12-31 23:15:00    NaN           44.0   \n",
       "74039518  126P04663  2022-12-31 23:30:00    NaN           44.0   \n",
       "74039519  126P04663  2022-12-31 23:45:00    NaN           44.0   \n",
       "\n",
       "          reference_speed  travel_time_seconds data_density       road  \\\n",
       "0                     NaN                  NaN          NaN  BISHOP ST   \n",
       "1                     NaN                  NaN          NaN  BISHOP ST   \n",
       "2                     NaN                  NaN          NaN  BISHOP ST   \n",
       "3                     NaN                  NaN          NaN  BISHOP ST   \n",
       "4                     NaN                  NaN          NaN  BISHOP ST   \n",
       "...                   ...                  ...          ...        ...   \n",
       "74039515             53.0                  NaN          NaN      HI-19   \n",
       "74039516             53.0                  NaN          NaN      HI-19   \n",
       "74039517             53.0                  NaN          NaN      HI-19   \n",
       "74039518             53.0                  NaN          NaN      HI-19   \n",
       "74039519             53.0                  NaN          NaN      HI-19   \n",
       "\n",
       "                       data_origin  \n",
       "0         npmrds_from_inrix_trucks  \n",
       "1         npmrds_from_inrix_trucks  \n",
       "2         npmrds_from_inrix_trucks  \n",
       "3         npmrds_from_inrix_trucks  \n",
       "4         npmrds_from_inrix_trucks  \n",
       "...                            ...  \n",
       "74039515  npmrds_from_inrix_trucks  \n",
       "74039516  npmrds_from_inrix_trucks  \n",
       "74039517  npmrds_from_inrix_trucks  \n",
       "74039518  npmrds_from_inrix_trucks  \n",
       "74039519  npmrds_from_inrix_trucks  \n",
       "\n",
       "[74039520 rows x 9 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['main_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8895da6a-1be3-45e9-8152-21ed76402b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_datetime_columns(main_data):\n",
    "    '''\n",
    "    Generates an actual datetime column in the \"main_data\" DataFrame by parsing \n",
    "    the text-based timestamp column. Also extracts day-of-week and time info \n",
    "    into separate columns.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        The pandas DataFrame that contains all the raw data from the RITIS\n",
    "        website (INRIX/NPMRDS speeds)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    main_data : pd.DataFrame\n",
    "        The same DataFrame as the input, except that now, the DataFrame has a \n",
    "        few new datetime-related columns. Namely:\n",
    "            -day_of_week: indicates the row's day of the week as a number from \n",
    "                0 (Monday) to 6 (Sunday)\n",
    "            -day_of_week_str: indicates the row's day of the week as a string\n",
    "                of text\n",
    "            -time: indicates the row's TIME (without date)\n",
    "\n",
    "    '''\n",
    "    # Transforming STRING timestamp into an actual datetime format\n",
    "    main_data['measurement_tstamp'] = pd.to_datetime(main_data['measurement_tstamp'])\n",
    "    \n",
    "    # Extracting day-of-week data and making it more readable.\n",
    "    # Monday=0, Sunday=6\n",
    "    main_data['day_of_week'] = main_data.measurement_tstamp.dt.day_of_week\n",
    "    main_data['day_of_week_str'] = (main_data['day_of_week']\n",
    "                                    .apply(lambda x: {0:'0 - Monday',\n",
    "                                                      1:'1 - Tuesday',\n",
    "                                                      2:'2 - Wednesday',\n",
    "                                                      3:'3 - Thursday',\n",
    "                                                      4:'4 - Friday',\n",
    "                                                      5:'5 - Saturday',\n",
    "                                                      6:'6 - Sunday'}[x]))\n",
    "    \n",
    "    \n",
    "    # Extracting day-of-year data\n",
    "    main_data['day_of_year'] = main_data.measurement_tstamp.dt.day_of_year\n",
    "    \n",
    "    # Extracting the time value, which was coded originally in 15 minute intervals\n",
    "    main_data['time'] = main_data.measurement_tstamp.dt.time\n",
    "    \n",
    "    return main_data\n",
    "\n",
    "class time_slot():\n",
    "    '''\n",
    "    Class that is used to label the observations in the `main_data` DataFrame \n",
    "    (that contains all the raw data from the RITIS speeds database) according \n",
    "    to the time of day. For example: am_peak, pm_peak, etc.\n",
    "    ''' \n",
    "    def __init__(self,time_start,time_end,include_start, include_end, \n",
    "                 inside_outside,slot_name):\n",
    "        '''\n",
    "        Instantiates `time_slot`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        time_start : datetime.time\n",
    "            Start time of the time slot\n",
    "        time_end : datetime.time\n",
    "            End time of the time slot\n",
    "        include_start : BOOL\n",
    "            Indicates whether to use >= or just > for time_start\n",
    "        include_end : BOOL\n",
    "            Indicates whether to use <= or just < for time_end\n",
    "        inside_outside : STR\n",
    "            Indicates whether the time slot refers to the time inside or outside\n",
    "            of the start and end times. To be more specific:\n",
    "                If inside_outside==\"inside\", then the time slot refers to the \n",
    "                time AFTER time_start but BEFORE time_end. \n",
    "                If inside_outside==\"outside\" , then the time slot refers to the \n",
    "                time BEFORE time_start but AFTER time_end (e.g.: before 6am and \n",
    "                after 10pm). \n",
    "        slot_name : STR\n",
    "            Describes the name of the time slot. Typical names include \"am_peak\",\n",
    "            \"pm_peak\", \"off_peak\".\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The newly-created instance of this class.\n",
    "\n",
    "        '''\n",
    "        self.time_start     = time_start\n",
    "        self.time_end       = time_end\n",
    "        self.include_start  = include_start\n",
    "        self.include_end    = include_end\n",
    "        self.inside_outside = inside_outside\n",
    "        self.slot_name      = slot_name\n",
    "        \n",
    "    \n",
    "    def get_filter(self, main_data):\n",
    "        '''\n",
    "        Gets the filter/mask that indicates which of the INRIX observations belong\n",
    "        to this specific time slot.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        main_data : pd.DataFrame\n",
    "            The pandas DataFrame that contains all the raw data from the RITIS\n",
    "            website (INRIX/NPMRDS speeds)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        ts_filter : pd.Series (bool)\n",
    "            An array of BOOL variables that indicates whether or not each \n",
    "            observation belongs to this specific time slot. The array has length\n",
    "            equal to the number of rows in main_data.\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            return self.ts_filter\n",
    "        except: \n",
    "            if self.inside_outside == 'inside':\n",
    "                this_filter = ((self.time_start < main_data['time']) & \n",
    "                               (main_data['time'] < self.time_end))\n",
    "            elif self.inside_outside == 'outside':\n",
    "                this_filter = ((main_data['time'] < self.time_start) | \n",
    "                               (self.time_end < main_data['time']))\n",
    "            if self.include_start:\n",
    "                this_filter = (this_filter | \n",
    "                               (main_data['time'] == self.time_start))\n",
    "            if self.include_end:\n",
    "                this_filter = (this_filter | \n",
    "                               (main_data['time'] == self.time_end))\n",
    "            self.ts_filter = this_filter\n",
    "            return self.ts_filter\n",
    "        \n",
    "    def add_time_slot_data_to_main_data(self, main_data):\n",
    "        '''\n",
    "        Adds the 'time_slot' column to the data and applies `time_slot`'s name\n",
    "        to the appropriate rows.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        main_data : pd.DataFrame\n",
    "            The pandas DataFrame that contains all the raw data from the RITIS\n",
    "            website (INRIX/NPMRDS speeds)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        main_data : pd.DataFrame\n",
    "            The same DataFrame as was passed in the input. The only difference \n",
    "            is that now, the `time_slot`'s name was applied to the rows that \n",
    "            fall within the `time_slot`'s filter.\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            main_data.loc[self.get_filter,'time_slot'] = self.slot_name\n",
    "            return main_data\n",
    "        except:\n",
    "            main_data['time_slot'] = np.nan\n",
    "            main_data.loc[self.get_filter,'time_slot'] = self.slot_name\n",
    "            return main_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5970525b-8b48-4eb3-8cfc-4df4faa53d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_data = test['main_data']\n",
    "main_tmc_data = test['main_tmc_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "854b9065-4795-4233-8f2c-fe889914aa3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 1min 11s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main_data = fix_datetime_columns(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1cf910e4-c773-40c6-bf9b-926fa56c16fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmc_code</th>\n",
       "      <th>measurement_tstamp</th>\n",
       "      <th>speed</th>\n",
       "      <th>average_speed</th>\n",
       "      <th>reference_speed</th>\n",
       "      <th>travel_time_seconds</th>\n",
       "      <th>data_density</th>\n",
       "      <th>road</th>\n",
       "      <th>data_origin</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_week_str</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 00:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>00:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 00:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 00:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126N04514</td>\n",
       "      <td>2022-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BISHOP ST</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>1</td>\n",
       "      <td>01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039515</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 22:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>365</td>\n",
       "      <td>22:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039516</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>365</td>\n",
       "      <td>23:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039517</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 23:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>365</td>\n",
       "      <td>23:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039518</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 23:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>365</td>\n",
       "      <td>23:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74039519</th>\n",
       "      <td>126P04663</td>\n",
       "      <td>2022-12-31 23:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>44.0</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-19</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>5</td>\n",
       "      <td>5 - Saturday</td>\n",
       "      <td>365</td>\n",
       "      <td>23:45:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74039520 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tmc_code  measurement_tstamp  speed  average_speed  \\\n",
       "0         126N04514 2022-01-01 00:00:00    NaN            NaN   \n",
       "1         126N04514 2022-01-01 00:15:00    NaN            NaN   \n",
       "2         126N04514 2022-01-01 00:30:00    NaN            NaN   \n",
       "3         126N04514 2022-01-01 00:45:00    NaN            NaN   \n",
       "4         126N04514 2022-01-01 01:00:00    NaN            NaN   \n",
       "...             ...                 ...    ...            ...   \n",
       "74039515  126P04663 2022-12-31 22:45:00    NaN           46.0   \n",
       "74039516  126P04663 2022-12-31 23:00:00    NaN           44.0   \n",
       "74039517  126P04663 2022-12-31 23:15:00    NaN           44.0   \n",
       "74039518  126P04663 2022-12-31 23:30:00    NaN           44.0   \n",
       "74039519  126P04663 2022-12-31 23:45:00    NaN           44.0   \n",
       "\n",
       "          reference_speed  travel_time_seconds data_density       road  \\\n",
       "0                     NaN                  NaN          NaN  BISHOP ST   \n",
       "1                     NaN                  NaN          NaN  BISHOP ST   \n",
       "2                     NaN                  NaN          NaN  BISHOP ST   \n",
       "3                     NaN                  NaN          NaN  BISHOP ST   \n",
       "4                     NaN                  NaN          NaN  BISHOP ST   \n",
       "...                   ...                  ...          ...        ...   \n",
       "74039515             53.0                  NaN          NaN      HI-19   \n",
       "74039516             53.0                  NaN          NaN      HI-19   \n",
       "74039517             53.0                  NaN          NaN      HI-19   \n",
       "74039518             53.0                  NaN          NaN      HI-19   \n",
       "74039519             53.0                  NaN          NaN      HI-19   \n",
       "\n",
       "                       data_origin  day_of_week day_of_week_str  day_of_year  \\\n",
       "0         npmrds_from_inrix_trucks            5    5 - Saturday            1   \n",
       "1         npmrds_from_inrix_trucks            5    5 - Saturday            1   \n",
       "2         npmrds_from_inrix_trucks            5    5 - Saturday            1   \n",
       "3         npmrds_from_inrix_trucks            5    5 - Saturday            1   \n",
       "4         npmrds_from_inrix_trucks            5    5 - Saturday            1   \n",
       "...                            ...          ...             ...          ...   \n",
       "74039515  npmrds_from_inrix_trucks            5    5 - Saturday          365   \n",
       "74039516  npmrds_from_inrix_trucks            5    5 - Saturday          365   \n",
       "74039517  npmrds_from_inrix_trucks            5    5 - Saturday          365   \n",
       "74039518  npmrds_from_inrix_trucks            5    5 - Saturday          365   \n",
       "74039519  npmrds_from_inrix_trucks            5    5 - Saturday          365   \n",
       "\n",
       "              time  \n",
       "0         00:00:00  \n",
       "1         00:15:00  \n",
       "2         00:30:00  \n",
       "3         00:45:00  \n",
       "4         01:00:00  \n",
       "...            ...  \n",
       "74039515  22:45:00  \n",
       "74039516  23:00:00  \n",
       "74039517  23:15:00  \n",
       "74039518  23:30:00  \n",
       "74039519  23:45:00  \n",
       "\n",
       "[74039520 rows x 13 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e541a153-e209-423d-b7ad-62689461a05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class day_of_year_window():\n",
    "    '''\n",
    "    Class that is used to label the observations in the `main_data` DataFrame \n",
    "    (that contains all the raw data from the RITIS speeds database) according \n",
    "    to the day of the year. For example: some analyses require only data between \n",
    "    September and October.\n",
    "    ''' \n",
    "    def __init__(self,start_date,end_date,include_start, include_end, \n",
    "                 inside_outside,window_name):\n",
    "        '''\n",
    "        Instantiates `day_of_year_window`.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        start_date : INT\n",
    "            Integer that indicates the day_of_year for the window's start date\n",
    "        end_date : INT\n",
    "            Integer that indicates the day_of_year for the window's end date\n",
    "        include_start : BOOL\n",
    "            Indicates whether to use >= or just > for start_date\n",
    "        include_end : BOOL\n",
    "            Indicates whether to use <= or just < for end_date\n",
    "        inside_outside : STR\n",
    "            Indicates whether the window refers to the days inside or outside\n",
    "            of the start and end dates. To be more specific:\n",
    "                If inside_outside==\"inside\", then the window refers to the \n",
    "                time AFTER start_date but BEFORE end_date. \n",
    "                If inside_outside==\"outside\" , then the time slot refers to the \n",
    "                time BEFORE start_date but AFTER end_date (e.g.: Before January\n",
    "                25th and after November 12th). \n",
    "        window_name : STR\n",
    "            Describes the name of the window.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        The newly-created instance of this class.\n",
    "\n",
    "        '''\n",
    "        self.start_date     = start_date\n",
    "        self.end_date       = end_date\n",
    "        self.include_start  = include_start\n",
    "        self.include_end    = include_end\n",
    "        self.inside_outside = inside_outside\n",
    "        self.window_name    = window_name\n",
    "        \n",
    "    \n",
    "    def get_filter(self, main_data):\n",
    "        '''\n",
    "        Gets the filter/mask that indicates which of the INRIX observations belong\n",
    "        to this specific date window.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        main_data : pd.DataFrame\n",
    "            The pandas DataFrame that contains all the raw data from the RITIS\n",
    "            website (INRIX/NPMRDS speeds)\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        doy_filter : pd.Series (bool)\n",
    "            An array of BOOL variables that indicates whether or not each \n",
    "            observation belongs to this specific window. The array has length\n",
    "            equal to the number of rows in main_data.\n",
    "        '''\n",
    "        \n",
    "        try:\n",
    "            return self.doy_filter\n",
    "        except: \n",
    "            if self.inside_outside == 'inside':\n",
    "                this_filter = ((self.start_date < main_data['day_of_year']) &\n",
    "                               (main_data['day_of_year'] < self.end_date))\n",
    "            elif self.inside_outside == 'outside':\n",
    "                this_filter = ((main_data['day_of_year'] < self.start_date) | \n",
    "                               (self.end_date < main_data['day_of_year']))\n",
    "            if self.include_start:\n",
    "                this_filter = (this_filter | \n",
    "                               (main_data['day_of_year'] == self.start_date))\n",
    "            if self.include_end:\n",
    "                this_filter = (this_filter | \n",
    "                               (main_data['day_of_year'] == self.end_date))\n",
    "            self.doy_filter = this_filter\n",
    "            return self.doy_filter\n",
    "        \n",
    "    def add_window_data_to_main_data(self, main_data):\n",
    "        '''\n",
    "        Adds the 'date_window' column to the data and applies the \n",
    "        `day_of_year_window`'s name to the appropriate rows.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        main_data : pd.DataFrame\n",
    "            The pandas DataFrame that contains all the raw data from the RITIS\n",
    "            website (INRIX/NPMRDS speeds)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        main_data : pd.DataFrame\n",
    "            The same DataFrame as was passed in the input. The only difference \n",
    "            is that now, the `day_of_year_window`'s name was applied to the rows \n",
    "            that fall within the `day_of_year_window`'s filter.\n",
    "\n",
    "        '''\n",
    "        try:\n",
    "            main_data.loc[self.get_filter,'date_window'] = self.window_name\n",
    "            return main_data\n",
    "        except:\n",
    "            main_data['date_window'] = np.nan\n",
    "            main_data.loc[self.get_filter,'date_window'] = self.window_name\n",
    "            return main_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54cd334c-ecdb-46c3-a150-26902fc9a370",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_summaries(grouped_data):\n",
    "    '''\n",
    "    Calculates all the important summaries for means and percentiles.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    grouped_data : DataFrameGroupBy object\n",
    "        DataFrame that was filtered down and grouped using the `groupby` function.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    grouped_data_summaries : pd.DataFrame\n",
    "        Pandas DataFrame containing all of the summary results: means and \n",
    "        percentiles for speed and travel time.\n",
    "\n",
    "    '''\n",
    "    # Determining which column name to use: minutes or seconds\n",
    "    if 'travel_time_minutes' in grouped_data.head().columns:\n",
    "        tt_col = 'travel_time_minutes'\n",
    "    else:\n",
    "        tt_col = 'travel_time_seconds'\n",
    "    \n",
    "    grouped_data_summaries = grouped_data.agg(\n",
    "        count_obs = pd.NamedAgg(column='measurement_tstamp', aggfunc='count'),\n",
    "                                                \n",
    "        speed_avg = pd.NamedAgg(column='speed', aggfunc=('mean')),\n",
    "        speed_01p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q =  1))),\n",
    "        speed_05p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q =  5))),\n",
    "        speed_15p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 15))),\n",
    "        speed_20p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 20))),\n",
    "        speed_50p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 50))),\n",
    "        speed_80p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 80))),\n",
    "        speed_85p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 85))),\n",
    "        speed_95p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 95))),\n",
    "        speed_99p = pd.NamedAgg(column='speed', aggfunc=(lambda x: np.percentile(x, q = 99))),\n",
    "        \n",
    "        ttime_avg = pd.NamedAgg(column=tt_col, aggfunc=('mean')),\n",
    "        ttime_01p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q =  1))),\n",
    "        ttime_05p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q =  5))),\n",
    "        ttime_15p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 15))),\n",
    "        ttime_20p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 20))),\n",
    "        ttime_50p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 50))),\n",
    "        ttime_80p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 80))),\n",
    "        ttime_85p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 85))),\n",
    "        ttime_95p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 95))),\n",
    "        ttime_99p = pd.NamedAgg(column=tt_col, aggfunc=(lambda x: np.percentile(x, q = 99))),\n",
    "        \n",
    "        )\n",
    "    return grouped_data_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "45747bc2-8524-4065-bf5f-58b9127c41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_summaries_pipeline(main_data,\n",
    "                            summary_name,\n",
    "                            summary_filter,\n",
    "                            grouping_columns):\n",
    "    '''\n",
    "    This function simplifies and standardizes the process of calculating \n",
    "    summaries from the raw data. The user needs to tell the function what\n",
    "    rows are to be kept, what columns will be used to group the data and the \n",
    "    name of this particular summary.\n",
    "    This function then returns the newly-calculated summarized data containing\n",
    "    a new column called \"summary_type\".\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        The pandas DataFrame that contains all the raw data from the RITIS\n",
    "        website (INRIX/NPMRDS speeds).\n",
    "    summary_name : str\n",
    "        String that describes this summary type. After the `main_data` is \n",
    "        summarized, a new column called \"summary_type\" will be generated. \n",
    "        This new column will contain the text stored in the `summary_name`\n",
    "        variable.\n",
    "    summary_filter : np.array\n",
    "        Array containing only boolean values (False/True). This indicates which\n",
    "        rows from the `main_data` should be used in the calculation of these \n",
    "        summaries.\n",
    "    grouping_columns : list\n",
    "        List of column names that will be used to group the `main_data`\n",
    "        dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    summarized_data : pd.DataFrame\n",
    "        Pandas DataFrame containing all of the summary results: means and \n",
    "        percentiles for speed and travel time.\n",
    "\n",
    "    '''\n",
    "    grouped_data = main_data.loc[summary_filter].groupby(grouping_columns)\n",
    "    \n",
    "    summarized_data = calc_summaries(grouped_data)\n",
    "    \n",
    "    summarized_data['summary_type'] = summary_name\n",
    "    \n",
    "    return summarized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4760e-6f32-4880-869e-38ca3e309cfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97dc8c9b-6732-43d8-abba-794fac5acc85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d102082-dacc-4874-85fa-3a5c9ded9049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b3ea7ac-8bd3-4cf5-b495-c2b8453ea4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_npmrds_geodata(npmrds_geodata_path):\n",
    "    '''\n",
    "    Reads in the shapefile associated with the NPMRDS data. Typically, this \n",
    "    file is just called \"Texas.shp\"\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    npmrds_geodata_path : STR\n",
    "        String describing the full path to the \".shp\" file on disk of where the \n",
    "        NPMRDS data can be found. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    npmrds_geodata : gpd.GeoDataFrame\n",
    "        The GeoDataFrame with the actual geodata from the NPMRDS shapefile.\n",
    "    '''\n",
    "    \n",
    "    npmrds_geodata = gpd.read_file(npmrds_geodata_path).rename({'Tmc':'tmc_code'},\n",
    "                                                               axis=1)\n",
    "    \n",
    "    return npmrds_geodata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8b2199e4-c918-4692-b680-d0d176d7e59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_link(df_row):\n",
    "    '''\n",
    "    Function that creates a simplified link geometry (straight line) using the \n",
    "    start/end long/lat data from the original INRIX main data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_row : pd.Series\n",
    "        One row of the `main_data_summaries` DataFrame\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    row_line : shapely.LineString\n",
    "        Geometric feature created using the lat/lon data from in the input row\n",
    "\n",
    "    '''\n",
    "    start_pt = shapely.geometry.Point([df_row['start_longitude'], \n",
    "                                       df_row['start_latitude']])\n",
    "    end_pt = shapely.geometry.Point([df_row['end_longitude'], \n",
    "                                     df_row['end_latitude']])\n",
    "    row_line = shapely.geometry.LineString([start_pt,end_pt])\n",
    "    return row_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4aaafb6c-072a-44d8-9441-1538ebf90b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_geometries_to_summaries(summarized_data, \n",
    "                                main_tmc_data,\n",
    "                                npmrds_geodata_path):\n",
    "    '''\n",
    "    Adds a column called \"geom_final\" to the dataset. This new column contains \n",
    "    a geometry for each row in the summary dataset. This geometry is generated \n",
    "    in one of two different ways, in the following priority:\n",
    "        -Look in the NPMRDS shapefile to try and find a link with matching TMC\n",
    "        -If we can't find one, we just draw a straight line from the lat/lon\n",
    "            data that is found in the TMC_Identification.csv files.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    summarized_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the speed summaries. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the \n",
    "        TMC segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "    npmrds_geodata_path : STR\n",
    "        String that identifies where to find the NPMRDS shapefile. Needs to \n",
    "        point to the \".shp\" file. Typically, this file is just called \"Texas.shp\"\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    summarized_data_with_geoms : gpd.GeoDataFrame\n",
    "        GeoDataFrame containing the geometries associated with each link.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    npmrds_geodata = gpd.read_file(npmrds_geodata_path).rename({'Tmc':'tmc_code'},\n",
    "                                                               axis=1)\n",
    "    \n",
    "    # Merging summaries with NPMRDS geometries\n",
    "    summarized_data_with_geoms = summarized_data.merge(\n",
    "        npmrds_geodata[['tmc_code','geometry']].to_crs('epsg:4326'), \n",
    "        how='left', \n",
    "        on='tmc_code')\n",
    "    \n",
    "    # Applying the \"make_link\" function to generate the simplified geometries\n",
    "    temp_geoms = (summarized_data_with_geoms\n",
    "                  .drop_duplicates(subset=['data_origin','tmc_code'])\n",
    "                  .sort_values(by=['data_origin','tmc_code'])\n",
    "                  .reset_index(drop=True))\n",
    "    \n",
    "    temp_geoms = temp_geoms.merge(main_tmc_data[['tmc_code',\n",
    "                                                 'data_origin',\n",
    "                                                 'start_latitude',\n",
    "                                                 'start_longitude',\n",
    "                                                 'end_latitude',\n",
    "                                                 'end_longitude']], \n",
    "                                  how='left', \n",
    "                                  on=['data_origin','tmc_code'])\n",
    "    \n",
    "    temp_geoms['geometry_simplified'] = temp_geoms.apply(make_link, axis=1)\n",
    "\n",
    "    summarized_data_with_geoms = summarized_data_with_geoms.merge(\n",
    "        temp_geoms[['data_origin','tmc_code','geometry_simplified']], \n",
    "        how='left', on=['data_origin','tmc_code'])\n",
    "    \n",
    "    \n",
    "    # Filter that indicates which observations/rows did not have an \n",
    "    # associated geometry in the NPMRDS geodata.\n",
    "    null_geoms_filter = summarized_data_with_geoms['geometry'].isnull()\n",
    "    \n",
    "    # Populating the `geom_final` column with either the geometry found in the NPMRDS\n",
    "    # geodata (preferred), or with the simplified geometry created above (fallback).\n",
    "    summarized_data_with_geoms['geom_final'] = summarized_data_with_geoms['geometry']\n",
    "    summarized_data_with_geoms.loc[null_geoms_filter,'geom_final'] = (\n",
    "        summarized_data_with_geoms.loc[null_geoms_filter,'geometry_simplified'])\n",
    "    \n",
    "    # Populating the `geom_final_type` column with a flag that shows what type of \n",
    "    # geometry is in the `geom_final` column. \n",
    "    # If `geom_final_type`== 'original_tmc_shape', then the geometry in \n",
    "    # `geom_final` represents the original NPMRDS geometry. \n",
    "    # If `geom_final_type`== 'simplified_tmc_shape', then the geometry in \n",
    "    # `geom_final` represents the simplified geometry we created in this script\n",
    "    # just a few lines above. \n",
    "    summarized_data_with_geoms['geom_final_type'] = 'original_tmc_shape'\n",
    "    summarized_data_with_geoms.loc[null_geoms_filter,\n",
    "                                   'geom_final_type'] = 'simplified_tmc_shape'\n",
    "    \n",
    "    summarized_data_with_geoms = gpd.GeoDataFrame(\n",
    "        summarized_data_with_geoms.drop(['geometry','geometry_simplified']\n",
    "                                        ,axis=1),\n",
    "        crs='epsg:4326',geometry='geom_final')\n",
    "    \n",
    "    # Extracting the WKT data. Useful for exporting to CSV. \n",
    "    summarized_data_with_geoms['geom_wkt'] = gpd.array.to_wkt(\n",
    "        summarized_data_with_geoms.geom_final.values)\n",
    "    \n",
    "    # Exporting final data to disk\n",
    "    #summarized_data_with_geoms.to_file(main_data_geoms_filename,driver='GPKG',layer='main')    \n",
    "\n",
    "    return summarized_data_with_geoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a5f5a05f-c7e3-4f18-a96e-0e0c9829462a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_standard_fhwa_timeslots(main_data):\n",
    "    '''\n",
    "    Adds timeslot and date window columns to `main_data`. These are the \n",
    "    standard timeslots used for the FHWA reliability computations:\n",
    "        -AM Peak:   Between 06am and 10am\n",
    "        -Mid-day:   Between 10am and 04pm\n",
    "        -PM Peak:   Between 04pm and 08pm\n",
    "        -Overnight: Between 08pm and 06am\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        Input DataFrame containing raw speed data for all links.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    main_data : pd.DataFrame\n",
    "        DataFrame containing raw speed data for all links. \n",
    "        After this function is executed, the following columns get added:\n",
    "            -\"time_slot\"\n",
    "            -\"date_window\"\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # Creating the thresholds for the definitions of peak and off-peak time slots\n",
    "    # and finding the observations that fall in each category/time slot.\n",
    "    # Time slots used are:\n",
    "    #    -AM Peak:   Between 06am and 10am\n",
    "    #    -Mid-day:   Between 10am and 04pm\n",
    "    #    -PM Peak:   Between 04pm and 08pm\n",
    "    #    -Overnight: Between 08pm and 06am\n",
    "    \n",
    "    am_peak = time_slot(time_start = pd.to_datetime('06:00 AM').time(),\n",
    "                        time_end = pd.to_datetime('10:00 AM').time(),\n",
    "                        include_start = True, \n",
    "                        include_end = False,\n",
    "                        inside_outside = 'inside',\n",
    "                        slot_name = 'am_peak')\n",
    "    \n",
    "    afternoon_offpeak = time_slot(time_start = pd.to_datetime('10:00 AM').time(),\n",
    "                                  time_end = pd.to_datetime('04:00 PM').time(),\n",
    "                                  include_start = True, \n",
    "                                  include_end = False,\n",
    "                                  inside_outside = 'inside',\n",
    "                                  slot_name = 'midday')\n",
    "    \n",
    "    pm_peak = time_slot(time_start = pd.to_datetime('04:00 PM').time(),\n",
    "                        time_end = pd.to_datetime('08:00 PM').time(),\n",
    "                        include_start = True, \n",
    "                        include_end = False,\n",
    "                        inside_outside = 'inside',\n",
    "                        slot_name = 'pm_peak')\n",
    "    \n",
    "    night = time_slot(time_start = pd.to_datetime('06:00 AM').time(),\n",
    "                      time_end = pd.to_datetime('08:00 PM').time(),\n",
    "                      include_start = False, \n",
    "                      include_end = True,\n",
    "                      inside_outside = 'outside',\n",
    "                      slot_name = 'overnight')\n",
    "    \n",
    "    all_time_slots = [am_peak, afternoon_offpeak, pm_peak, night]\n",
    "    \n",
    "    # Adding the peak/offpeak/etc category data back into the `main_data` DataFrame\n",
    "    for this_time_slot in all_time_slots:\n",
    "        main_data = this_time_slot.add_time_slot_data_to_main_data(main_data)\n",
    "    \n",
    "    # Creating the thresholds for the definitions of day-of-year windows and finding\n",
    "    # the observations that fall in each category/window.\n",
    "    # The windows used are:\n",
    "    #    -All days: Between Jan 1, 2019 and Dec 31, 2019\n",
    "    # Note: Currently, there is only one category that spans the entire year. \n",
    "    #       The functionality was built in for future projects, when we might want \n",
    "    #       to compare, say, speeds during the four seasons. \n",
    "    \n",
    "    all_days_window = day_of_year_window(start_date=pd.to_datetime('Jan 1, 2019').day_of_year, \n",
    "                                         end_date=pd.to_datetime('Dec 31, 2019').day_of_year, \n",
    "                                         include_start = True, \n",
    "                                         include_end = True,\n",
    "                                         inside_outside = 'inside',\n",
    "                                         window_name = 'all_days')\n",
    "    \n",
    "    main_data = all_days_window.add_window_data_to_main_data(main_data)\n",
    "    \n",
    "    return main_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "712cae4a-0fbb-4380-b80f-8a1a28249d60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmc_code</th>\n",
       "      <th>measurement_tstamp</th>\n",
       "      <th>speed</th>\n",
       "      <th>average_speed</th>\n",
       "      <th>reference_speed</th>\n",
       "      <th>travel_time_seconds</th>\n",
       "      <th>data_density</th>\n",
       "      <th>road</th>\n",
       "      <th>data_origin</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_week_str</th>\n",
       "      <th>day_of_year</th>\n",
       "      <th>time</th>\n",
       "      <th>time_slot</th>\n",
       "      <th>date_window</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>126N04516</td>\n",
       "      <td>2017-01-01 00:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NUUANU AVE</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>126N04516</td>\n",
       "      <td>2017-01-01 00:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NUUANU AVE</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>00:15:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>126N04516</td>\n",
       "      <td>2017-01-01 00:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NUUANU AVE</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>00:30:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126N04516</td>\n",
       "      <td>2017-01-01 00:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NUUANU AVE</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>00:45:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>126N04516</td>\n",
       "      <td>2017-01-01 01:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NUUANU AVE</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>1</td>\n",
       "      <td>01:00:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65343739</th>\n",
       "      <td>126P04661</td>\n",
       "      <td>2017-12-31 22:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-56</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>365</td>\n",
       "      <td>22:45:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65343740</th>\n",
       "      <td>126P04661</td>\n",
       "      <td>2017-12-31 23:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-56</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>365</td>\n",
       "      <td>23:00:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65343741</th>\n",
       "      <td>126P04661</td>\n",
       "      <td>2017-12-31 23:15:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-56</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>365</td>\n",
       "      <td>23:15:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65343742</th>\n",
       "      <td>126P04661</td>\n",
       "      <td>2017-12-31 23:30:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-56</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>365</td>\n",
       "      <td>23:30:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65343743</th>\n",
       "      <td>126P04661</td>\n",
       "      <td>2017-12-31 23:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>HI-56</td>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>6</td>\n",
       "      <td>6 - Sunday</td>\n",
       "      <td>365</td>\n",
       "      <td>23:45:00</td>\n",
       "      <td>overnight</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65343744 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           tmc_code  measurement_tstamp  speed  average_speed  \\\n",
       "0         126N04516 2017-01-01 00:00:00    NaN            NaN   \n",
       "1         126N04516 2017-01-01 00:15:00    NaN            NaN   \n",
       "2         126N04516 2017-01-01 00:30:00    NaN            NaN   \n",
       "3         126N04516 2017-01-01 00:45:00    NaN            NaN   \n",
       "4         126N04516 2017-01-01 01:00:00    NaN            NaN   \n",
       "...             ...                 ...    ...            ...   \n",
       "65343739  126P04661 2017-12-31 22:45:00    NaN           39.0   \n",
       "65343740  126P04661 2017-12-31 23:00:00    NaN           42.0   \n",
       "65343741  126P04661 2017-12-31 23:15:00    NaN           42.0   \n",
       "65343742  126P04661 2017-12-31 23:30:00    NaN           42.0   \n",
       "65343743  126P04661 2017-12-31 23:45:00    NaN           42.0   \n",
       "\n",
       "          reference_speed  travel_time_seconds data_density        road  \\\n",
       "0                    28.0                  NaN          NaN  NUUANU AVE   \n",
       "1                    28.0                  NaN          NaN  NUUANU AVE   \n",
       "2                    28.0                  NaN          NaN  NUUANU AVE   \n",
       "3                    28.0                  NaN          NaN  NUUANU AVE   \n",
       "4                    28.0                  NaN          NaN  NUUANU AVE   \n",
       "...                   ...                  ...          ...         ...   \n",
       "65343739             50.0                  NaN          NaN       HI-56   \n",
       "65343740             50.0                  NaN          NaN       HI-56   \n",
       "65343741             50.0                  NaN          NaN       HI-56   \n",
       "65343742             50.0                  NaN          NaN       HI-56   \n",
       "65343743             50.0                  NaN          NaN       HI-56   \n",
       "\n",
       "                       data_origin  day_of_week day_of_week_str  day_of_year  \\\n",
       "0         npmrds_from_inrix_trucks            6      6 - Sunday            1   \n",
       "1         npmrds_from_inrix_trucks            6      6 - Sunday            1   \n",
       "2         npmrds_from_inrix_trucks            6      6 - Sunday            1   \n",
       "3         npmrds_from_inrix_trucks            6      6 - Sunday            1   \n",
       "4         npmrds_from_inrix_trucks            6      6 - Sunday            1   \n",
       "...                            ...          ...             ...          ...   \n",
       "65343739  npmrds_from_inrix_trucks            6      6 - Sunday          365   \n",
       "65343740  npmrds_from_inrix_trucks            6      6 - Sunday          365   \n",
       "65343741  npmrds_from_inrix_trucks            6      6 - Sunday          365   \n",
       "65343742  npmrds_from_inrix_trucks            6      6 - Sunday          365   \n",
       "65343743  npmrds_from_inrix_trucks            6      6 - Sunday          365   \n",
       "\n",
       "              time  time_slot date_window  \n",
       "0         00:00:00  overnight    all_days  \n",
       "1         00:15:00  overnight    all_days  \n",
       "2         00:30:00  overnight    all_days  \n",
       "3         00:45:00  overnight    all_days  \n",
       "4         01:00:00  overnight    all_days  \n",
       "...            ...        ...         ...  \n",
       "65343739  22:45:00  overnight    all_days  \n",
       "65343740  23:00:00  overnight    all_days  \n",
       "65343741  23:15:00  overnight    all_days  \n",
       "65343742  23:30:00  overnight    all_days  \n",
       "65343743  23:45:00  overnight    all_days  \n",
       "\n",
       "[65343744 rows x 15 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "define_standard_fhwa_timeslots(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "b4d96965-ad23-483a-9313-f4dffc3e18f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 38.3 s\n",
      "Wall time: 37.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "main_data = define_standard_fhwa_timeslots(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b0e8a7-db43-4441-bcd0-24412233869e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0c335868-72f9-4c9d-b25c-5bfa93c2f5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filters for Weekdays. Peak and afternoon-off-peak times.\n",
    "time_slot_filter   = main_data['time_slot'].isin(['am_peak'])\n",
    "day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "time_filter        = np.repeat(True, len(main_data))\n",
    "day_of_week_filter = main_data['day_of_week'].isin([0,1,2,3,4])\n",
    "    \n",
    "# Combining all the filters\n",
    "summary_filter = (time_slot_filter & \n",
    "                      day_of_year_filter & \n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "grouping_columns = ['data_origin','tmc_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "6fe91798-527d-488a-91ed-6db60f3bd74e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x0000019282A76C50>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_data = main_data.loc[summary_filter].groupby(grouping_columns)\n",
    "grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c690cef2-7b06-4d23-83f4-b191d88c394e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>count_obs</th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>speed_01p</th>\n",
       "      <th>speed_05p</th>\n",
       "      <th>speed_15p</th>\n",
       "      <th>speed_20p</th>\n",
       "      <th>speed_50p</th>\n",
       "      <th>speed_80p</th>\n",
       "      <th>speed_85p</th>\n",
       "      <th>speed_95p</th>\n",
       "      <th>...</th>\n",
       "      <th>ttime_avg</th>\n",
       "      <th>ttime_01p</th>\n",
       "      <th>ttime_05p</th>\n",
       "      <th>ttime_15p</th>\n",
       "      <th>ttime_20p</th>\n",
       "      <th>ttime_50p</th>\n",
       "      <th>ttime_80p</th>\n",
       "      <th>ttime_85p</th>\n",
       "      <th>ttime_95p</th>\n",
       "      <th>ttime_99p</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>data_origin</th>\n",
       "      <th>tmc_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"11\" valign=\"top\">npmrds_from_inrix_trucks</th>\n",
       "      <th>126+04098</th>\n",
       "      <td>4160</td>\n",
       "      <td>39.465676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>6.766911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126+04099</th>\n",
       "      <td>4160</td>\n",
       "      <td>39.695578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>39.613949</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126+04100</th>\n",
       "      <td>4160</td>\n",
       "      <td>44.484172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>57.152976</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126+04101</th>\n",
       "      <td>4160</td>\n",
       "      <td>42.245109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>70.743545</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126+04102</th>\n",
       "      <td>4160</td>\n",
       "      <td>39.585800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>34.898510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126P50036</th>\n",
       "      <td>4160</td>\n",
       "      <td>15.212810</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.164857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126P50039</th>\n",
       "      <td>4160</td>\n",
       "      <td>25.285714</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>11.071429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126P50041</th>\n",
       "      <td>4160</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.310000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126P50044</th>\n",
       "      <td>4160</td>\n",
       "      <td>14.437500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15.808125</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126P50045</th>\n",
       "      <td>4160</td>\n",
       "      <td>16.312083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7.900312</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2113 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    count_obs  speed_avg  speed_01p  \\\n",
       "data_origin              tmc_code                                     \n",
       "npmrds_from_inrix_trucks 126+04098       4160  39.465676        NaN   \n",
       "                         126+04099       4160  39.695578        NaN   \n",
       "                         126+04100       4160  44.484172        NaN   \n",
       "                         126+04101       4160  42.245109        NaN   \n",
       "                         126+04102       4160  39.585800        NaN   \n",
       "...                                       ...        ...        ...   \n",
       "                         126P50036       4160  15.212810        NaN   \n",
       "                         126P50039       4160  25.285714        NaN   \n",
       "                         126P50041       4160  25.000000        NaN   \n",
       "                         126P50044       4160  14.437500        NaN   \n",
       "                         126P50045       4160  16.312083        NaN   \n",
       "\n",
       "                                    speed_05p  speed_15p  speed_20p  \\\n",
       "data_origin              tmc_code                                     \n",
       "npmrds_from_inrix_trucks 126+04098        NaN        NaN        NaN   \n",
       "                         126+04099        NaN        NaN        NaN   \n",
       "                         126+04100        NaN        NaN        NaN   \n",
       "                         126+04101        NaN        NaN        NaN   \n",
       "                         126+04102        NaN        NaN        NaN   \n",
       "...                                       ...        ...        ...   \n",
       "                         126P50036        NaN        NaN        NaN   \n",
       "                         126P50039        NaN        NaN        NaN   \n",
       "                         126P50041        NaN        NaN        NaN   \n",
       "                         126P50044        NaN        NaN        NaN   \n",
       "                         126P50045        NaN        NaN        NaN   \n",
       "\n",
       "                                    speed_50p  speed_80p  speed_85p  \\\n",
       "data_origin              tmc_code                                     \n",
       "npmrds_from_inrix_trucks 126+04098        NaN        NaN        NaN   \n",
       "                         126+04099        NaN        NaN        NaN   \n",
       "                         126+04100        NaN        NaN        NaN   \n",
       "                         126+04101        NaN        NaN        NaN   \n",
       "                         126+04102        NaN        NaN        NaN   \n",
       "...                                       ...        ...        ...   \n",
       "                         126P50036        NaN        NaN        NaN   \n",
       "                         126P50039        NaN        NaN        NaN   \n",
       "                         126P50041        NaN        NaN        NaN   \n",
       "                         126P50044        NaN        NaN        NaN   \n",
       "                         126P50045        NaN        NaN        NaN   \n",
       "\n",
       "                                    speed_95p  ...  ttime_avg  ttime_01p  \\\n",
       "data_origin              tmc_code              ...                         \n",
       "npmrds_from_inrix_trucks 126+04098        NaN  ...   6.766911        NaN   \n",
       "                         126+04099        NaN  ...  39.613949        NaN   \n",
       "                         126+04100        NaN  ...  57.152976        NaN   \n",
       "                         126+04101        NaN  ...  70.743545        NaN   \n",
       "                         126+04102        NaN  ...  34.898510        NaN   \n",
       "...                                       ...  ...        ...        ...   \n",
       "                         126P50036        NaN  ...   7.164857        NaN   \n",
       "                         126P50039        NaN  ...  11.071429        NaN   \n",
       "                         126P50041        NaN  ...   3.310000        NaN   \n",
       "                         126P50044        NaN  ...  15.808125        NaN   \n",
       "                         126P50045        NaN  ...   7.900312        NaN   \n",
       "\n",
       "                                    ttime_05p  ttime_15p  ttime_20p  \\\n",
       "data_origin              tmc_code                                     \n",
       "npmrds_from_inrix_trucks 126+04098        NaN        NaN        NaN   \n",
       "                         126+04099        NaN        NaN        NaN   \n",
       "                         126+04100        NaN        NaN        NaN   \n",
       "                         126+04101        NaN        NaN        NaN   \n",
       "                         126+04102        NaN        NaN        NaN   \n",
       "...                                       ...        ...        ...   \n",
       "                         126P50036        NaN        NaN        NaN   \n",
       "                         126P50039        NaN        NaN        NaN   \n",
       "                         126P50041        NaN        NaN        NaN   \n",
       "                         126P50044        NaN        NaN        NaN   \n",
       "                         126P50045        NaN        NaN        NaN   \n",
       "\n",
       "                                    ttime_50p  ttime_80p  ttime_85p  \\\n",
       "data_origin              tmc_code                                     \n",
       "npmrds_from_inrix_trucks 126+04098        NaN        NaN        NaN   \n",
       "                         126+04099        NaN        NaN        NaN   \n",
       "                         126+04100        NaN        NaN        NaN   \n",
       "                         126+04101        NaN        NaN        NaN   \n",
       "                         126+04102        NaN        NaN        NaN   \n",
       "...                                       ...        ...        ...   \n",
       "                         126P50036        NaN        NaN        NaN   \n",
       "                         126P50039        NaN        NaN        NaN   \n",
       "                         126P50041        NaN        NaN        NaN   \n",
       "                         126P50044        NaN        NaN        NaN   \n",
       "                         126P50045        NaN        NaN        NaN   \n",
       "\n",
       "                                    ttime_95p  ttime_99p  \n",
       "data_origin              tmc_code                         \n",
       "npmrds_from_inrix_trucks 126+04098        NaN        NaN  \n",
       "                         126+04099        NaN        NaN  \n",
       "                         126+04100        NaN        NaN  \n",
       "                         126+04101        NaN        NaN  \n",
       "                         126+04102        NaN        NaN  \n",
       "...                                       ...        ...  \n",
       "                         126P50036        NaN        NaN  \n",
       "                         126P50039        NaN        NaN  \n",
       "                         126P50041        NaN        NaN  \n",
       "                         126P50044        NaN        NaN  \n",
       "                         126P50045        NaN        NaN  \n",
       "\n",
       "[2113 rows x 21 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarized_data = calc_summaries(grouped_data)\n",
    "summarized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd79867f-9fc4-470a-8dcb-5ce33b427c19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e069b7d-68ec-4ff8-8b59-6436a3c64996",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cd3631bf-a191-493a-aae3-3217af217a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_group_summarize_fhwa(main_data):\n",
    "    '''\n",
    "    Defines the standard periods and summaries needed for calculating the \n",
    "    FHWA reliability values.\n",
    "    To see the formal definitions of these periods, see CFR 23 490.511 and \n",
    "    CFR 23 490.611:\n",
    "        https://www.ecfr.gov/current/title-23/chapter-I/subchapter-E/part-490/subpart-E/section-490.511\n",
    "        https://www.ecfr.gov/current/title-23/chapter-I/subchapter-E/part-490/subpart-F/section-490.611\n",
    "        https://www.law.cornell.edu/cfr/text/23/490.511\n",
    "        https://www.law.cornell.edu/cfr/text/23/490.611\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        Input dataframe containing raw speed data for every time period. \n",
    "        It is expected that this dataframe will contain the following columns:\n",
    "            -\"time_slot\"\n",
    "            -\"date_window\"\n",
    "            -\"day_of_week\"\n",
    "            -\"time\"\n",
    "            -\"day_of_year\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    all_summaries_concat : pd.DataFrame\n",
    "        DataFrame that contains all the standard summary data required for \n",
    "        FHWA's reliability calculations.\n",
    "\n",
    "    '''\n",
    "    \n",
    "    #--------------------------------------------------------\n",
    "    # Step 1: Summarizing data for AM Peaks (only weekdays) -\n",
    "    #--------------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'am_peak'\n",
    "    \n",
    "    # Filters for Weekdays. Peak and afternoon-off-peak times.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['am_peak'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = main_data['day_of_week'].isin([0,1,2,3,4])\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year_filter & \n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_ampeak = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    \n",
    "    #-------------------------------------------------------\n",
    "    # Step 2: Summarizing data for Mid-day (only weekdays) -\n",
    "    #-------------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'midday'\n",
    "    \n",
    "    # Filters for Weekdays. Peak and afternoon-off-peak times.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['midday'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = main_data['day_of_week'].isin([0,1,2,3,4])\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year_filter & \n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_midday = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    \n",
    "    #--------------------------------------------------------\n",
    "    # Step 3: Summarizing data for PM Peaks (only weekdays) -\n",
    "    #--------------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'pm_peak'\n",
    "    \n",
    "    # Filters for Weekdays. Peak and afternoon-off-peak times.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['pm_peak'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = main_data['day_of_week'].isin([0,1,2,3,4])\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year_filter & \n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_pmpeak = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    \n",
    "    #----------------------------------------------------\n",
    "    # Step 4: Summarizing data for overnight - all days -\n",
    "    #----------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'overnight'\n",
    "    \n",
    "    # Filters for Weekends - only considering 6am to 8pm.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['overnight'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = np.repeat(True, len(main_data))\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                        day_of_year_filter & \n",
    "                        time_filter & \n",
    "                        day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_overnight = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    \n",
    "    #-----------------------------------------------------\n",
    "    # Step 5: Summarizing data for weekends - 6am to 8pm -\n",
    "    #-----------------------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'weekends'\n",
    "    \n",
    "    # Filters for Weekends - only considering 6am to 8pm.\n",
    "    time_slot_filter   = main_data['time_slot'].isin(['am_peak','midday','pm_peak'])\n",
    "    day_of_year_filter = main_data['date_window'].isin(['all_days'])\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = main_data['day_of_week'].isin([5,6])\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year_filter & \n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    # Calculating the summaries\n",
    "    summarized_data_weekends = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    # Adding extra detail about this summary's time slot\n",
    "    #summarized_data_weekends['time_slot'] = '6am_to_8pm'\n",
    "    \n",
    "    \n",
    "    #--------------------------------------\n",
    "    # Step 6: Summarizing data - All-time -\n",
    "    #--------------------------------------\n",
    "    \n",
    "    # Name for this summary\n",
    "    summary_name = 'alltime'\n",
    "    \n",
    "    # Filters for alltime averages\n",
    "    time_slot_filter   = np.repeat(True, len(main_data))\n",
    "    day_of_year        = np.repeat(True, len(main_data))\n",
    "    time_filter        = np.repeat(True, len(main_data))\n",
    "    day_of_week_filter = np.repeat(True, len(main_data))\n",
    "    \n",
    "    # Combining all the filters\n",
    "    summary_filter = (time_slot_filter & \n",
    "                      day_of_year &\n",
    "                      time_filter & \n",
    "                      day_of_week_filter)\n",
    "    \n",
    "    # TODO: Need to find a more user-friendly way to define these filters\n",
    "    \n",
    "    # Columns used to group data for summaries\n",
    "    grouping_columns = ['data_origin','tmc_code']\n",
    "    \n",
    "    summarized_data_alltime = calc_summaries_pipeline(\n",
    "        main_data=main_data, \n",
    "        summary_name=summary_name,\n",
    "        summary_filter=summary_filter,\n",
    "        grouping_columns=grouping_columns)\n",
    "    \n",
    "    # Adding extra detail about this summary's time slot\n",
    "    #summarized_data_alltime['time_slot'] = 'all_hours'\n",
    "    \n",
    "    # Creating list with all the summaries from the previous step\n",
    "    all_summaries = [summarized_data_ampeak.reset_index(),\n",
    "                     summarized_data_midday.reset_index(),\n",
    "                     summarized_data_pmpeak.reset_index(),\n",
    "                     summarized_data_overnight.reset_index(),\n",
    "                     summarized_data_weekends.reset_index(),\n",
    "                     summarized_data_alltime.reset_index()]\n",
    "    \n",
    "    # Concatenating all of the summaries into one large DataFrame\n",
    "    all_summaries_concat = pd.concat(all_summaries, ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    return all_summaries_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "31b48db0-9db5-43f0-bae4-adad58d24e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 4min 20s\n",
      "Wall time: 4min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_summaries_concat = filter_group_summarize_fhwa(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "9f3beac1-cb48-4f61-a732-0facb24c9e23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_origin</th>\n",
       "      <th>tmc_code</th>\n",
       "      <th>count_obs</th>\n",
       "      <th>speed_avg</th>\n",
       "      <th>speed_01p</th>\n",
       "      <th>speed_05p</th>\n",
       "      <th>speed_15p</th>\n",
       "      <th>speed_20p</th>\n",
       "      <th>speed_50p</th>\n",
       "      <th>speed_80p</th>\n",
       "      <th>...</th>\n",
       "      <th>ttime_01p</th>\n",
       "      <th>ttime_05p</th>\n",
       "      <th>ttime_15p</th>\n",
       "      <th>ttime_20p</th>\n",
       "      <th>ttime_50p</th>\n",
       "      <th>ttime_80p</th>\n",
       "      <th>ttime_85p</th>\n",
       "      <th>ttime_95p</th>\n",
       "      <th>ttime_99p</th>\n",
       "      <th>summary_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126+04098</td>\n",
       "      <td>4160</td>\n",
       "      <td>39.465676</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>am_peak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126+04099</td>\n",
       "      <td>4160</td>\n",
       "      <td>39.695578</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>am_peak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126+04100</td>\n",
       "      <td>4160</td>\n",
       "      <td>44.484172</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>am_peak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126+04101</td>\n",
       "      <td>4160</td>\n",
       "      <td>42.245109</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>am_peak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126+04102</td>\n",
       "      <td>4160</td>\n",
       "      <td>39.585800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>am_peak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12673</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126P50036</td>\n",
       "      <td>35040</td>\n",
       "      <td>14.738623</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12674</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126P50039</td>\n",
       "      <td>35040</td>\n",
       "      <td>20.367347</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12675</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126P50041</td>\n",
       "      <td>35040</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12676</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126P50044</td>\n",
       "      <td>35040</td>\n",
       "      <td>13.564706</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12677</th>\n",
       "      <td>npmrds_from_inrix_trucks</td>\n",
       "      <td>126P50045</td>\n",
       "      <td>35040</td>\n",
       "      <td>21.773651</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all_days</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12678 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    data_origin   tmc_code  count_obs  speed_avg  speed_01p  \\\n",
       "0      npmrds_from_inrix_trucks  126+04098       4160  39.465676        NaN   \n",
       "1      npmrds_from_inrix_trucks  126+04099       4160  39.695578        NaN   \n",
       "2      npmrds_from_inrix_trucks  126+04100       4160  44.484172        NaN   \n",
       "3      npmrds_from_inrix_trucks  126+04101       4160  42.245109        NaN   \n",
       "4      npmrds_from_inrix_trucks  126+04102       4160  39.585800        NaN   \n",
       "...                         ...        ...        ...        ...        ...   \n",
       "12673  npmrds_from_inrix_trucks  126P50036      35040  14.738623        NaN   \n",
       "12674  npmrds_from_inrix_trucks  126P50039      35040  20.367347        NaN   \n",
       "12675  npmrds_from_inrix_trucks  126P50041      35040  15.000000        NaN   \n",
       "12676  npmrds_from_inrix_trucks  126P50044      35040  13.564706        NaN   \n",
       "12677  npmrds_from_inrix_trucks  126P50045      35040  21.773651        NaN   \n",
       "\n",
       "       speed_05p  speed_15p  speed_20p  speed_50p  speed_80p  ...  ttime_01p  \\\n",
       "0            NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "1            NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "2            NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "3            NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "4            NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "...          ...        ...        ...        ...        ...  ...        ...   \n",
       "12673        NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "12674        NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "12675        NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "12676        NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "12677        NaN        NaN        NaN        NaN        NaN  ...        NaN   \n",
       "\n",
       "       ttime_05p  ttime_15p  ttime_20p  ttime_50p  ttime_80p  ttime_85p  \\\n",
       "0            NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1            NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2            NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3            NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4            NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "12673        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "12674        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "12675        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "12676        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "12677        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "       ttime_95p  ttime_99p  summary_type  \n",
       "0            NaN        NaN       am_peak  \n",
       "1            NaN        NaN       am_peak  \n",
       "2            NaN        NaN       am_peak  \n",
       "3            NaN        NaN       am_peak  \n",
       "4            NaN        NaN       am_peak  \n",
       "...          ...        ...           ...  \n",
       "12673        NaN        NaN      all_days  \n",
       "12674        NaN        NaN      all_days  \n",
       "12675        NaN        NaN      all_days  \n",
       "12676        NaN        NaN      all_days  \n",
       "12677        NaN        NaN      all_days  \n",
       "\n",
       "[12678 rows x 24 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_summaries_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bb5b9-909d-4af2-a2f7-d0c89ca6a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_tmc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd6401e0-6615-428d-9b48-cc57316f3b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_standard_reliabily_mixed_traffic(all_summaries_concat, \n",
    "                                               main_tmc_data):\n",
    "    '''\n",
    "    Calculates travel time reliability for mixed traffic according to FHWA's \n",
    "    standards. \n",
    "    Note: See \"National Performance Measures for Congestion, Reliability, and \n",
    "    Freight, and CMAQ Traffic Congestion\":\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/hif18040.pdf\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_summaries_concat : pd.DataFrame\n",
    "        Dataframe containing all of the summary data needed for the computation\n",
    "        of the reliability metrics. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the TMC\n",
    "        segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reliability_summaries_all : pd.DataFrame\n",
    "        Dataframe containing the reliability data for each TMC segment.\n",
    "\n",
    "    '''\n",
    "    # For the mixed traffic data (i.e., for observations where \n",
    "    # data_origin is in ['inrix', 'npmrds_from_inrix_trucks_and_passveh']):\n",
    "    #    Calculate 80th percentile divided by 50th percentile for four summary \n",
    "    #    groups: am_peak, midday, pm_peak, weekends.\n",
    "    #    Then, take the highest one of all four. If that value is larger than \n",
    "    #    1.5, the segment is deemd \"unreliable\". Otherwise, the segment is \n",
    "    #    deemed \"reliable\".\n",
    "    \n",
    "\n",
    "    # Keeping only the relevant summaries\n",
    "    mixed_traffic_data = all_summaries_concat.loc[\n",
    "        (all_summaries_concat['summary_type'].isin(['am_peak', \n",
    "                                                    'midday', \n",
    "                                                    'pm_peak',\n",
    "                                                    'weekends'])) \n",
    "        & (all_summaries_concat['data_origin'].isin(['inrix',\n",
    "                                                     'npmrds_from_inrix_trucks_and_passveh']))\n",
    "        ].reset_index(drop=True)\n",
    "    \n",
    "    # Calculating Level of Travel Time Reliability: 80th percentile divided by \n",
    "    # 50th percentile (travel times)\n",
    "    mixed_traffic_data['LOTTR_80p_by_50p'] = (\n",
    "        mixed_traffic_data['ttime_80p'] / \n",
    "        mixed_traffic_data['ttime_50p'])\n",
    "    \n",
    "    # Calculating the maximum of the Travel Time Reliability across all \n",
    "    # summaries\n",
    "    reliability_summaries_mixed_traffic = (mixed_traffic_data\n",
    "        .groupby(['tmc_code','data_origin'])\n",
    "        .agg(\n",
    "            RawDataRows = pd.NamedAgg(column='count_obs', \n",
    "                                       aggfunc='sum'),\n",
    "            SummaryCount = pd.NamedAgg(column='LOTTR_80p_by_50p', \n",
    "                                       aggfunc='count'),\n",
    "            Reliability = pd.NamedAgg(column='LOTTR_80p_by_50p', \n",
    "                                                   aggfunc='max'))\n",
    "        .reset_index())\n",
    "    \n",
    "    reliability_summaries_mixed_traffic['Reliability_Type'] = 'Mixed_Traffic'\n",
    "    \n",
    "    # Dropping rows that didn't have summaries for all periods needed\n",
    "    reliability_summaries_mixed_traffic = (\n",
    "        reliability_summaries_mixed_traffic.loc[\n",
    "            reliability_summaries_mixed_traffic['SummaryCount'] == 4]\n",
    "        .reset_index(drop=True))\n",
    "    \n",
    "    # Adding the binary \"Reliable\" column. \n",
    "    reliability_summaries_mixed_traffic['Reliable'] = (\n",
    "        reliability_summaries_mixed_traffic['Reliability'] < 1.5)\n",
    "    \n",
    "    reliability_summaries_mixed_traffic = (reliability_summaries_mixed_traffic\n",
    "                                           [['tmc_code', 'data_origin', \n",
    "                                             'Reliability_Type', 'RawDataRows', \n",
    "                                             'SummaryCount', 'Reliability',\n",
    "                                             'Reliable']])\n",
    "    \n",
    "    return reliability_summaries_mixed_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9ac21520-e328-455b-8748-fae9ebf5e751",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_standard_reliability_trucks(all_summaries_concat, main_tmc_data):\n",
    "    '''\n",
    "    Calculates truck travel time reliability according to FHWA's standards. \n",
    "    Note: See \"National Performance Measures for Congestion, Reliability, and \n",
    "    Freight, and CMAQ Traffic Congestion\":\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/hif18040.pdf\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    all_summaries_concat : pd.DataFrame\n",
    "        Dataframe containing all of the summary data needed for the computation\n",
    "        of the reliability metrics. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the TMC\n",
    "        segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reliability_summaries_all : pd.DataFrame\n",
    "        Dataframe containing the reliability data for each TMC segment.\n",
    "\n",
    "    '''\n",
    "    # For truck traffic (data_origin='npmrds_from_inrix_trucks'):\n",
    "    #    Calculate 95th percentile divided by 50th percentile for the five \n",
    "    #    summary groups: am_peak, midday, pm_peak, overnight and weekends.\n",
    "    #    Then, just take the maximum of those five values. \n",
    "    #\n",
    "    # Note: Please note that, for trucks, we also have to analyze the \n",
    "    #       \"overnight\" summary, which is absent in the mixed traffic \n",
    "    #       summaries.\n",
    "    \n",
    "    # Keeping only the relevant summaries\n",
    "    truck_data = all_summaries_concat.loc[\n",
    "        (all_summaries_concat['summary_type'].isin(['am_peak', \n",
    "                                                    'midday', \n",
    "                                                    'pm_peak',\n",
    "                                                    'overnight',\n",
    "                                                    'weekends'])) & \n",
    "        (all_summaries_concat['data_origin']=='npmrds_from_inrix_trucks')\n",
    "        ].reset_index(drop=True)\n",
    "    \n",
    "    # Calculating Level of Travel Time Reliability: 95th percentile divided by \n",
    "    # 50th percentile (travel times)\n",
    "    truck_data['LOTTR_95p_by_50p'] = (\n",
    "        truck_data['ttime_95p'] / \n",
    "        truck_data['ttime_50p'])\n",
    "    \n",
    "    # Calculating the maximum of the Travel Time Reliability across all \n",
    "    # summaries\n",
    "    reliability_summaries_truck_traffic = (truck_data\n",
    "        .groupby(['tmc_code','data_origin'])\n",
    "        .agg(\n",
    "            RawDataRows = pd.NamedAgg(column='count_obs', \n",
    "                                       aggfunc='sum'),\n",
    "            SummaryCount = pd.NamedAgg(column='LOTTR_95p_by_50p', \n",
    "                                       aggfunc='count'),\n",
    "            Reliability = pd.NamedAgg(column='LOTTR_95p_by_50p', \n",
    "                                                   aggfunc='max'))\n",
    "        .reset_index())\n",
    "\n",
    "    reliability_summaries_truck_traffic['Reliability_Type'] = 'Truck_Traffic'\n",
    "\n",
    "    # Dropping rows that didn't have summaries for all periods needed\n",
    "    reliability_summaries_truck_traffic = (\n",
    "        reliability_summaries_truck_traffic.loc[\n",
    "            reliability_summaries_truck_traffic['SummaryCount'] == 5]\n",
    "        .reset_index(drop=True))\n",
    "    \n",
    "    # Adding the binary \"Reliable\" column. \n",
    "    reliability_summaries_truck_traffic['Reliable'] = (\n",
    "        reliability_summaries_truck_traffic['Reliability'] < 1.5)\n",
    "    \n",
    "    reliability_summaries_truck_traffic = (reliability_summaries_truck_traffic\n",
    "                                           [['tmc_code', 'data_origin', \n",
    "                                             'Reliability_Type', 'RawDataRows', \n",
    "                                             'SummaryCount', 'Reliability',\n",
    "                                             'Reliable']])\n",
    "    \n",
    "    return reliability_summaries_truck_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b4ed24ee-fed2-4ea8-af74-6ec5488199de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_standard_reliabilities(all_summaries_concat, \n",
    "                                     main_data, \n",
    "                                     main_tmc_data,\n",
    "                                     calc_mixed_traf_rel=True,\n",
    "                                     calc_truck_rel=True):\n",
    "    '''\n",
    "    Calculates the overall Reliability according to FHWA's standards. \n",
    "    Note: See \"National Performance Measures for Congestion, Reliability, and \n",
    "    Freight, and CMAQ Traffic Congestion\":\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/\n",
    "        https://www.fhwa.dot.gov/tpm/guidance/hif18040.pdf\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    all_summaries_concat : pd.DataFrame\n",
    "        Dataframe containing all of the summary data needed for the computation\n",
    "        of the reliability metrics. \n",
    "    main_data : pd.DataFrame\n",
    "        Input dataframe containing raw speed data for every time period. \n",
    "    main_tmc_data : pd.DataFrame\n",
    "        Pandas DataFrame that contains the associated TMC data for all the TMC\n",
    "        segments (i.e., the data from all the \"TMC_Identification.csv\" files)\n",
    "    calc_mixed_traf_rel : boolean\n",
    "        Flag that signals whether or not to calculate reliability for mixed \n",
    "        traffic\n",
    "    calc_truck_rel : boolean\n",
    "        Flag that signals whether or not to calculate reliability for trucks\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reliability_summaries_all : pd.DataFrame\n",
    "        Dataframe containing the reliability data for each TMC segment.\n",
    "\n",
    "    '''\n",
    "    # Calculating overall Reliability. \n",
    "    \n",
    "    # For the mixed traffic data (data_origin='npmrds_from_inrix_trucks_and_passveh'):\n",
    "    #    Calculate 80th percentile divided by 50th percentile for four summary \n",
    "    #    groups: am_peak, midday, pm_peak, weekends.\n",
    "    #    Then, take the highest one of all four. If that value is larger than 1.5, \n",
    "    #    the segment is deemed \"unreliable\". Otherwise, the segment is deemed \"reliable\".\n",
    "    \n",
    "    # For truck traffic (data_origin='npmrds_from_inrix_trucks'):\n",
    "    #    Calculate 95th percentile divided by 50th percentile for the five summary\n",
    "    #    groups: am_peak, midday, pm_peak, overnight and weekends.\n",
    "    #    Then, just take the maximum of those five values. \n",
    "    #    Note: For trucks, this is a continuous value, while for mixed traffic the \n",
    "    #    metric is just a binary \"reliable\"/\"unreliable\" variable.\n",
    "    \n",
    "    # List that will store all reliability results\n",
    "    reliability_dfs = []\n",
    "    \n",
    "    # Calculating reliability for mixed traffic\n",
    "    if calc_mixed_traf_rel:\n",
    "        reliability_summaries_mixed_traffic = (\n",
    "            calculate_standard_reliabily_mixed_traffic(all_summaries_concat, \n",
    "                                                       main_tmc_data))\n",
    "        \n",
    "        # Finding missing TMC codes and re-including them\n",
    "        missing_tmc_codes_mixed_traf = find_missing_tmc_codes(main_data, \n",
    "                                                   reliability_summaries_mixed_traffic)\n",
    "        \n",
    "        missing_tmc_codes_mixed_traf_df = (\n",
    "            pd.DataFrame({'tmc_code':missing_tmc_codes_mixed_traf,\n",
    "                          'data_origin':reliability_summaries_mixed_traffic['data_origin'].values[0]}))\n",
    "        \n",
    "        reliability_summaries_mixed_traffic = (\n",
    "            pd.concat([reliability_summaries_mixed_traffic, \n",
    "                       missing_tmc_codes_mixed_traf_df],\n",
    "                      ignore_index=True).reset_index(drop=True))\n",
    "\n",
    "        reliability_dfs.append(reliability_summaries_mixed_traffic)\n",
    "    \n",
    "    # Calculating reliability for trucks\n",
    "    if calc_truck_rel:\n",
    "        reliability_summaries_truck_traffic = (\n",
    "            calculate_standard_reliability_trucks(all_summaries_concat, \n",
    "                                                  main_tmc_data))\n",
    "        \n",
    "        missing_tmc_codes_truck = find_missing_tmc_codes(main_data, \n",
    "                                                   reliability_summaries_truck_traffic)\n",
    "        # Finding missing TMC codes and re-including them\n",
    "        missing_tmc_codestruck_df = (\n",
    "                    pd.DataFrame({'tmc_code':missing_tmc_codes_truck,\n",
    "                                  'data_origin':reliability_summaries_truck_traffic['data_origin'].values[0]}))\n",
    "                \n",
    "        reliability_summaries_truck_traffic = (\n",
    "            pd.concat([reliability_summaries_truck_traffic, \n",
    "                       missing_tmc_codestruck_df],\n",
    "                      ignore_index=True).reset_index(drop=True))\n",
    "        \n",
    "        reliability_dfs.append(reliability_summaries_truck_traffic)\n",
    "    \n",
    "    # Combining mixed traffic and truck reliability data\n",
    "    reliability_summaries_all = pd.concat(reliability_dfs,\n",
    "                                          ignore_index=True).reset_index(drop=True)\n",
    "    \n",
    "    return reliability_summaries_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bc01d887-26d2-48b8-aa6e-74b117df1a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_tmc_codes(main_data, ref_data):\n",
    "    '''\n",
    "    Finds which TMC codes are missing in the `ref_data` from the original raw\n",
    "    dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    main_data : pd.DataFrame\n",
    "        Input dataframe containing raw speed data for every time period. \n",
    "    ref_data : pd.DataFrame\n",
    "        Reference data whose TMC codes will be checked against the original\n",
    "        raw data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    missing_tmc_codes : LIST\n",
    "        List of TMC codes that are \"missing\" from `ref_data` (i.e., they exist\n",
    "        in `main_data`, but not in `ref_data`).\n",
    "\n",
    "    '''\n",
    "    # Getting unique TMC codes from both sets\n",
    "    main_data_unique = main_data['tmc_code'].unique()\n",
    "    ref_data_unique  = set(ref_data['tmc_code'].unique())\n",
    "    \n",
    "    # List that will hold missing TMC codes\n",
    "    missing_tmc_codes = []\n",
    "    \n",
    "    # Comparing the sets and finding which ones are missing\n",
    "    trash = pd.Series(main_data_unique).apply(\n",
    "        lambda x: missing_tmc_codes.append(x) \n",
    "            if x not in ref_data_unique \n",
    "            else None)\n",
    "    \n",
    "    return missing_tmc_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f392e293-935c-4853-9f51-54452a492572",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_to_finish_fhwa_summaries_and_reliability(\n",
    "        input_data_folder='',\n",
    "        npmrds_geodata_path='',\n",
    "        road_str='',\n",
    "        chunk_size=100000,\n",
    "        export_raw_speed_data=False,\n",
    "        output_raw_data_filename_pickle='',\n",
    "        export_tmc_data=False,\n",
    "        output_tmc_data_filename_pickle='',\n",
    "        export_summary_data=False,\n",
    "        output_summary_data_filename_gpkg='',\n",
    "        export_reliability_data=False,\n",
    "        output_reliability_data_filename_gpkg=''):\n",
    "    '''\n",
    "    Does everything needed to calculate the FHWA reliability metrics from the \n",
    "    zipped raw data files from RITIS. \n",
    "    The several bells and whistles in this function's inputs are just controls\n",
    "    of whether or not to export some of the processed datasets to the local disk.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data_folder : STR\n",
    "        String that indicates the folder to be investigated for the zipfiles \n",
    "        containing the raw data\n",
    "    npmrds_geodata_path : STR\n",
    "        String that identifies where to find the NPMRDS shapefile. Needs to \n",
    "        point to the \".shp\" file. Typically, this file is just called \"Texas.shp\"\n",
    "    road_str: STR \n",
    "        String used to filter road segments based on their names. This is also \n",
    "        referred to as \"the main searched road\" in other places of this script.\n",
    "        The TMC segments will be filtered based on whether or not the 'road' \n",
    "        column contains this string. To get the entire dataset back, just use \n",
    "        an empty string ('').\n",
    "    chunk_size: INT\n",
    "        Integer used to determine number of rows read at a time by Pandas' \n",
    "        read_csv method.\n",
    "    export_raw_speed_data : bool\n",
    "        Determines whether or not to export the raw speed data to disk. \n",
    "        The default is False.\n",
    "    output_raw_data_filename_pickle : STR\n",
    "        Full (absolute) path of the PICKLE file containing the raw data\n",
    "        read in through this function.\n",
    "    export_tmc_data : bool\n",
    "        Determines whether or not to export the TMC Information data to disk. \n",
    "        The default is False.\n",
    "    output_tmc_data_filename_pickle : STR\n",
    "        Full (absolute) path of the PICKLE file containing the TMC data\n",
    "        read in through this function.\n",
    "    export_summary_data : bool\n",
    "        Determines whether or not to export the summary data to disk. \n",
    "        The default is False.\n",
    "    output_summary_data_filename_gpkg : STR\n",
    "        String that identifies the path and filename to give to the GeoDataFrame\n",
    "        that contains the summary data. This needs to be a GeoPackage\n",
    "        file ('.gpkg' extension).\n",
    "    export_reliability_data : bool\n",
    "        Determines whether or not to export the reliability data to disk. \n",
    "        The default is False.\n",
    "    output_reliability_data_filename_gpkg : STR\n",
    "        String that identifies the path and filename to give to the GeoDataFrame\n",
    "        that contains the reliability data. This needs to be a GeoPackage\n",
    "        file ('.gpkg' extension). The default is ''.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    output_dict : DICT\n",
    "        Dictionary containing four datasets:\n",
    "            -main_data: pd.DataFrame that contains all the raw data with the \n",
    "                extra processing columns \n",
    "            -main_tmc_data: pd.DataFrame that contains the TMC information\n",
    "                about all the links\n",
    "            -all_summaries_with_geoms: gpd.GeoDataFrame that contains all the\n",
    "                summary data\n",
    "            -reliability_summaries_with_geoms: gpd.GeoDataFrame that contains\n",
    "                all the reliability data\n",
    "    '''\n",
    "    \n",
    "    ############################\n",
    "    ### STEP 1: READING DATA ###\n",
    "    ############################\n",
    "    \n",
    "    # Actually reading in the whole data and generating the filtered output files\n",
    "    all_data = read_batch_of_raw_data(input_data_folder, \n",
    "                                      road_str, \n",
    "                                      chunk_size)\n",
    "    \n",
    "    # Fishing out the `main_data` and `main_tmc_data` DataFrames.\n",
    "    main_data = all_data['main_data']\n",
    "    main_tmc_data  = all_data['main_tmc_data']\n",
    "\n",
    "    # Exporting raw data and TMC information\n",
    "    if export_raw_speed_data:\n",
    "        main_data.to_pickle(output_raw_data_filename_pickle)\n",
    "    \n",
    "    if export_tmc_data:\n",
    "        main_tmc_data.to_pickle(output_tmc_data_filename_pickle)\n",
    "\n",
    "    ###################################################\n",
    "    ### STEP 2: PRE-POCESSING COLUMNS FOR FILTERING ###\n",
    "    ###################################################\n",
    "    \n",
    "    # Fixing datetime information: adding time and day_of_week columns\n",
    "    main_data = fix_datetime_columns(main_data)\n",
    "    \n",
    "    # Adding timeslot and date window columns\n",
    "    main_data = define_standard_fhwa_timeslots(main_data)\n",
    "    \n",
    "    ############################################################\n",
    "    ### STEP 3: FILTERING, GROUPING AND SUMMARIZING THE DATA ###\n",
    "    ############################################################\n",
    "    \n",
    "    all_summaries_concat = filter_group_summarize_fhwa(main_data)\n",
    "    \n",
    "    all_summaries_with_geoms = add_geometries_to_summaries(\n",
    "                                   summarized_data=all_summaries_concat, \n",
    "                                   main_tmc_data=main_tmc_data,\n",
    "                                   npmrds_geodata_path=npmrds_geodata_path)\n",
    "    \n",
    "    if export_summary_data:\n",
    "        all_summaries_with_geoms.to_file(output_summary_data_filename_gpkg, \n",
    "                                         driver='GPKG',\n",
    "                                         layer='main')\n",
    "    \n",
    "    ################################################\n",
    "    ### STEP 4: CALCULATING RELIABILITY MEASURES ###\n",
    "    ################################################\n",
    "    \n",
    "    reliability_summaries_all = calculate_standard_reliabilities(\n",
    "        all_summaries_concat=all_summaries_concat, \n",
    "        main_data=main_data, \n",
    "        main_tmc_data=main_tmc_data,\n",
    "        calc_mixed_traf_rel=True,\n",
    "        calc_truck_rel=True)\n",
    "    \n",
    "    reliability_summaries_with_geoms = add_geometries_to_summaries(\n",
    "        summarized_data=reliability_summaries_all, \n",
    "        main_tmc_data=main_tmc_data,\n",
    "        npmrds_geodata_path=npmrds_geodata_path)\n",
    "    \n",
    "    if export_reliability_data:\n",
    "        reliability_summaries_with_geoms.to_file(output_reliability_data_filename_gpkg, \n",
    "                                                 driver='GPKG',\n",
    "                                                 layer='main')\n",
    "    \n",
    "    output_dict = {'main_data':main_data,\n",
    "                   'main_tmc_data':main_tmc_data,\n",
    "                   'all_summaries_with_geoms':all_summaries_with_geoms,\n",
    "                   'reliability_summaries_with_geoms':reliability_summaries_with_geoms}\n",
    "    \n",
    "    return output_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa7a1c6-cf04-4279-922a-84bbbc1c0ba7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2a5ac8-8ea7-46d4-bf6f-75056f82b31c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "720b59df-8f2e-40de-8055-f54a0e74d570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 28 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmc_code</th>\n",
       "      <th>data_origin</th>\n",
       "      <th>Reliability_Type</th>\n",
       "      <th>RawDataRows</th>\n",
       "      <th>SummaryCount</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>Reliable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tmc_code, data_origin, Reliability_Type, RawDataRows, SummaryCount, Reliability, Reliable]\n",
       "Index: []"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "calculate_standard_reliabily_mixed_traffic(all_summaries_concat,main_tmc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e5be2132-ce88-4224-9dbc-5b43adb44bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 46.9 ms\n",
      "Wall time: 41.8 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmc_code</th>\n",
       "      <th>data_origin</th>\n",
       "      <th>Reliability_Type</th>\n",
       "      <th>RawDataRows</th>\n",
       "      <th>SummaryCount</th>\n",
       "      <th>Reliability</th>\n",
       "      <th>Reliable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tmc_code, data_origin, Reliability_Type, RawDataRows, SummaryCount, Reliability, Reliable]\n",
       "Index: []"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "calculate_standard_reliability_trucks(all_summaries_concat, main_tmc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce284945-9bd7-4583-b08d-08360f41252a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "calculate_standard_reliabilities(all_summaries_concat, \n",
    "                                     main_data, \n",
    "                                     main_tmc_data,\n",
    "                                     calc_mixed_traf_rel=True,\n",
    "                                     calc_truck_rel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3099ae10-8869-4a76-8a5a-4cbdc14e746e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
